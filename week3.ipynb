{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZwGwLg4DeZ+ufmdPzJy15",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varshapmmohanan/Varshapmmohanan/blob/main/week3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "WCWEcNcEkdjJ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/preprocessed_data (6).csv')"
      ],
      "metadata": {
        "id": "NJeTuv--kjv3"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the dataset has features and target variable\n",
        "X = data[['Annual_Income', 'Num_Bank_Accounts','Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Type_of_Loan','Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit','Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt','Credit_Utilization_Ratio', 'Credit_History_Age']] # Feature]\n",
        "y = data['Credit_Score']  # Target variable\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "MCiBRNIIlZc8"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "SXXzsLPj8nB-"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the dataset has features and target variable\n",
        "X = data[['Annual_Income', 'Num_Bank_Accounts','Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Type_of_Loan','Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit','Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt','Credit_Utilization_Ratio', 'Credit_History_Age']] # Feature]\n",
        "y = data['Credit_Score']  # Target variable\n",
        "\n",
        "# Split the dataset into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "id": "GvQ7XTGf8wsR"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "\n",
        "}\n",
        "\n",
        "# Initialize Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)"
      ],
      "metadata": {
        "id": "WYakCa6z81px"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Perform Grid Search with 3-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
        "                           param_grid=param_grid,\n",
        "                           cv=3,\n",
        "                           scoring='accuracy',\n",
        "                           n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate model performance on the test set\n",
        "test_accuracy = grid_search.best_estimator_.score(X_test, y_test)\n",
        "print(\"Test Accuracy with Best Model:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "7Nb9-IGM86Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the best model to make predictions on the validation set\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "y_val_pred = best_rf_model.predict(X_val)\n",
        "\n",
        "# Calculate accuracy on the validation set\n",
        "validation_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(\"Validation Accuracy with Best Model:\", validation_accuracy)"
      ],
      "metadata": {
        "id": "9aBvFi119Ck9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate model performance on the test set\n",
        "y_test_pred = best_rf_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(\"Test Accuracy with Best Model:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "Epdcjdcd7rvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "X_train = train_df[['Annual_Income', 'Num_Bank_Accounts','Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Type_of_Loan','Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit','Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt','Credit_Utilization_Ratio', 'Credit_History_Age']]\n",
        "y_train = train_df['Credit_Score']\n",
        "\n",
        "X_val = val_df[['Annual_Income', 'Num_Bank_Accounts','Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Type_of_Loan','Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit','Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt','Credit_Utilization_Ratio', 'Credit_History_Age']]\n",
        "y_val = val_df['Credit_Score']\n",
        "\n",
        "X_test = test_df[['Annual_Income', 'Num_Bank_Accounts','Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Type_of_Loan','Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit','Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt','Credit_Utilization_Ratio', 'Credit_History_Age']]\n",
        "y_test = test_df['Credit_Score']\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Perform Grid Search and evaluate model performance on validation set\n",
        "best_rf_model = (RandomForestClassifier(random_state=42)\n",
        "                 .fit(X_train, y_train))\n",
        "\n",
        "grid_search = (GridSearchCV(best_rf_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "               .fit(X_train, y_train))\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "validation_accuracy = grid_search.best_estimator_.score(X_val, y_val)\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Validation Accuracy with Best Model:\", validation_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "d48ukbBDrmTc",
        "outputId": "f46cc1ad-cccd-4c89-a1e9-26926bd5a1df"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-0b4fc6edf8c3>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m grid_search = (GridSearchCV(best_rf_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n\u001b[0;32m---> 24\u001b[0;31m                .fit(X_train, y_train))\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1706\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate model performance on validation set\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "validation_accuracy = best_rf_model.score(X_val, y_val)\n",
        "print(\"Validation Accuracy with Best Model:\", validation_accuracy)"
      ],
      "metadata": {
        "id": "muOZkiAKlypf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "# Assuming you have loaded your dataset and split it into X_train, X_test, y_train, y_test\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1.0, 10.0],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],              # Penalty type: L1 (Lasso) or L2 (Ridge)\n",
        "    'solver': ['liblinear', 'saga']      # Solver algorithm for optimization\n",
        "}\n",
        "\n",
        "# Initialize the Logistic Regression classifier\n",
        "log_reg_classifier = LogisticRegression(random_state=42)\n",
        "\n",
        "# Perform Grid Search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=log_reg_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Use the best model to make predictions on the test set\n",
        "best_log_reg_model = grid_search.best_estimator_\n",
        "y_pred = best_log_reg_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "shkL6W3Wl33j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_classifier(y_true, y_pred, label):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    print(f\"{label} Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"{label} Precision: {precision:.4f}\")\n",
        "    print(f\"{label} Recall: {recall:.4f}\")\n",
        "    print(f\"{label} F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Evaluate the classifier on the training, validation, and test sets\n",
        "evaluate_classifier(y_train, y_train_pred_lr, \"Training\")"
      ],
      "metadata": {
        "id": "kfdORg5wnmkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the classifier on the training, validation, and test sets\n",
        "evaluate_classifier(y_val, y_val_pred_lr, \"Validation\")"
      ],
      "metadata": {
        "id": "x4tOQyfvns-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the classifier on the training, validation, and test sets\n",
        "evaluate_classifier(y_test, y_test_pred_lr, \"Test\")"
      ],
      "metadata": {
        "id": "XoFOpH7kny-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range of C values you want to visualize\n",
        "C_values = [0.01, 0.1, 1, 10, 100]\n",
        "# Initialize empty lists to store performance metrics\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "# Loop through different C values\n",
        "for c in C_values:\n",
        "    lr_model = LogisticRegression(multi_class='multinomial', C=c, solver='newton-cg', max_iter=10, random_state=42)\n",
        "    lr_model.fit(X_train, y_train)\n",
        "    # Predict on different sets\n",
        "    y_train_pred = lr_model.predict(X_train)\n",
        "    y_val_pred = lr_model.predict(X_val)\n",
        "    y_test_pred = lr_model.predict(X_test)\n",
        "    # Calculate accuracy\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)"
      ],
      "metadata": {
        "id": "HeL_amdan4K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Append accuracies to lists\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(C_values, train_accuracies, label='Training Accuracy', marker = 'o')\n",
        "plt.plot(C_values, val_accuracies, label='Validation Accuracy', marker = 'o')\n",
        "plt.plot(C_values, test_accuracies, label='Test Accuracy', marker = 'o')\n",
        "plt.xlabel('C Value')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Impact of Hyperparameter C on Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pe0zTsfTn-Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solver_values = ['newton-cg', 'lbfgs', 'sag', 'saga']\n",
        "# Initialize empty lists to store performance metrics\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "# Loop through different C values\n",
        "for s in solver_values:\n",
        "    lr_model = LogisticRegression(multi_class='multinomial', C=10, solver=s, max_iter=10, random_state=42)\n",
        "    lr_model.fit(X_train, y_train)\n",
        "    # Predict on different sets\n",
        "    y_train_pred = lr_model.predict(X_train)\n",
        "    y_val_pred = lr_model.predict(X_val)\n",
        "    y_test_pred = lr_model.predict(X_test)\n",
        "    # Calculate accuracy\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    # Append accuracies to lists\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(solver_values, train_accuracies, label='Training Accuracy', marker = 'o')\n",
        "plt.plot(solver_values, val_accuracies, label='Validation Accuracy', marker = 'o')\n",
        "plt.plot(solver_values, test_accuracies, label='Test Accuracy', marker = 'o')\n",
        "plt.xlabel('Solver')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Impact of Hyperparameter Solver on Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gt-dGV_foFgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# impact of max_iter on accuracy\n",
        "# Define the range of C values you want to visualize\n",
        "max_iter_values = [10, 50, 100, 200, 300, 500, 700, 1000]\n",
        "# Initialize empty lists to store performance metrics\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "# Loop through different C values\n",
        "for m in max_iter_values:\n",
        "    lr_model = LogisticRegression(multi_class='multinomial', C=10, solver='lbfgs', max_iter=m, random_state=42)\n",
        "    lr_model.fit(X_train, y_train)\n",
        "    # Predict on different sets\n",
        "    y_train_pred = lr_model.predict(X_train)\n",
        "    y_val_pred = lr_model.predict(X_val)\n",
        "    y_test_pred = lr_model.predict(X_test)\n",
        "    # Calculate accuracy\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "     # Append accuracies to lists\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(max_iter_values, train_accuracies, label='Training Accuracy', marker = 'o')\n",
        "plt.plot(max_iter_values, val_accuracies, label='Validation Accuracy', marker = 'o')\n",
        "plt.plot(max_iter_values, test_accuracies, label='Test Accuracy', marker = 'o')\n",
        "plt.xlabel('Max Iterations')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Impact of Hyperparameter Max Iterations on Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "efX23N8MoO8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifie"
      ],
      "metadata": {
        "id": "IM6iNJyGocst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [5, 10, 50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 15],\n",
        "    'min_samples_split': [2, 3, 4, 5],\n",
        "    'min_samples_leaf': [1, 2, 3, 4]\n",
        "}\n",
        "\n",
        "RF_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1)\n",
        "RF_grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_classifier_rf = RF_grid_search.best_estimator_\n",
        "best_hyperparameters_rf = RF_grid_search.best_params_\n",
        "y_train_pred_rf = best_classifier_rf.predict(X_train)\n",
        "y_val_pred_rf = best_classifier_rf.predict(X_val)\n",
        "y_test_pred_rf = best_classifier_rf.predict(X_test)\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters_rf)  # Best Hyperparameters"
      ],
      "metadata": {
        "id": "qLCXh_NloYOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the classifier on the training, validation, and test sets\n",
        "evaluate_classifier(y_train, y_train_pred_rf, \"Training\")"
      ],
      "metadata": {
        "id": "Fac2nCZDolkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the classifier on the training, validation, and test sets\n",
        "evaluate_classifier(y_val, y_val_pred_rf, \"Validation\")"
      ],
      "metadata": {
        "id": "NylghKCOopCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the classifier on the training, validation, and test sets\n",
        "evaluate_classifier(y_test, y_test_pred_rf, \"Test\")"
      ],
      "metadata": {
        "id": "v-yTnKNlotCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Importance\n",
        "# Get feature importances\n",
        "importances = best_classifier_rf.feature_importances_\n",
        "# Get the standard deviation of feature importances\n",
        "std = np.std([tree.feature_importances_ for tree in best_classifier_rf.estimators_], axis=0)\n",
        "# Get the indices of the features sorted by importance\n",
        "indices = np.argsort(importances)[::-1]\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), indices)\n",
        "plt.xlim([-1, X_train.shape[1]])\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Feature Importances')"
      ],
      "metadata": {
        "id": "imGzYEgRowox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range of n_estimators values you want to visualize\n",
        "n_estimators_values = [10, 50, 100, 200, 500]\n",
        "# Initialize empty lists to store performance metrics\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "# Loop through different n_estimators values\n",
        "for n in n_estimators_values:\n",
        "    rf_model = RandomForestClassifier(n_estimators=n, max_depth=10, min_samples_split=4, min_samples_leaf=1, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    # Predict on different sets\n",
        "    y_train_pred = rf_model.predict(X_train)\n",
        "    y_val_pred = rf_model.predict(X_val)\n",
        "    y_test_pred = rf_model.predict(X_test)\n",
        "    # Calculate accuracy\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    # Append accuracies to lists\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_estimators_values, train_accuracies, label='Training Accuracy', marker = 'o')\n",
        "plt.plot(n_estimators_values, val_accuracies, label='Validation Accuracy', marker = 'o')\n",
        "plt.plot(n_estimators_values, test_accuracies, label='Test Accuracy', marker = 'o')\n",
        "plt.xlabel('n_estimators Value')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Impact of Hyperparameter n_estimators on Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "GLeR-58do1HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "O2Uzjx3LpIAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range of max_depth values you want to visualize\n",
        "max_depth_values = [None, 5, 10, 15]\n",
        "# Initialize empty lists to store performance metrics\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "# Loop through different max_depth values\n",
        "for m in max_depth_values:\n",
        "    rf_model = RandomForestClassifier(n_estimators=200, max_depth=m, min_samples_split=4, min_samples_leaf=1, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    # Predict on different sets\n",
        "    y_train_pred = rf_model.predict(X_train)\n",
        "    y_val_pred = rf_model.predict(X_val)\n",
        "    y_test_pred = rf_model.predict(X_test)\n",
        "    # Calculate accuracy\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "     # Append accuracies to lists\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(max_depth_values, train_accuracies, label='Training Accuracy', marker = 'o')\n",
        "plt.plot(max_depth_values, val_accuracies, label='Validation Accuracy', marker = 'o')\n",
        "plt.plot(max_depth_values, test_accuracies, label='Test Accuracy', marker = 'o')\n",
        "plt.xlabel('Max Depth Value')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Impact of Hyperparameter Max Depth on Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Ory3cszTo_cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range of min_samples_split values you want to visualize\n",
        "min_samples_split_values = [2, 3, 4, 5]\n",
        "# Initialize empty lists to store performance metrics\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "# Loop through different min_samples_split values\n",
        "for m in min_samples_split_values:\n",
        "    rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=m, min_samples_leaf=1, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    # Predict on different sets\n",
        "    y_train_pred = rf_model.predict(X_train)\n",
        "    y_val_pred = rf_model.predict(X_val)\n",
        "    y_test_pred = rf_model.predict(X_test)\n",
        "    # Calculate accuracy\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    # Append accuracies to lists\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(min_samples_split_values, train_accuracies, label='Training Accuracy', marker = 'o')\n",
        "plt.plot(min_samples_split_values, val_accuracies, label='Validation Accuracy', marker = 'o')\n",
        "plt.plot(min_samples_split_values, test_accuracies, label='Test Accuracy', marker = 'o')\n",
        "plt.xlabel('Min Samples Split Value')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Impact of Hyperparameter Min Samples Split on Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "HleAEpXFpTtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range of min_samples_leaf values you want to visualize\n",
        "min_samples_leaf_values = [1, 2, 3, 4]\n",
        "# Initialize empty lists to store performance metrics\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "# Loop through different min_samples_leaf values\n",
        "for m in min_samples_leaf_values:\n",
        "    rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=4, min_samples_leaf=m, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    # Predict on different sets\n",
        "    y_train_pred = rf_model.predict(X_train)\n",
        "    y_val_pred = rf_model.predict(X_val)\n",
        "    y_test_pred = rf_model.predict(X_test)\n",
        "    # Calculate accuracy\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    # Append accuracies to lists\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(min_samples_leaf_values, train_accuracies, label='Training Accuracy', marker = 'o')\n",
        "plt.plot(min_samples_leaf_values, val_accuracies, label='Validation Accuracy', marker = 'o')\n",
        "plt.plot(min_samples_leaf_values, test_accuracies, label='Test Accuracy', marker = 'o')\n",
        "plt.xlabel('Min Samples Leaf Value')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Impact of Hyperparameter Min Samples Leaf on Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "suvbQWf1pexz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the classifiers into an ensemble\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Initialize the classifiers with the best hyperparameters\n",
        "log_reg_best = LogisticRegression(**best_hyperparameters_lr, multi_class='multinomial', random_state=42)\n",
        "rf_best = RandomForestClassifier(**best_hyperparameters_rf, random_state=42, n_jobs=1)\n",
        "\n",
        "# Create an ensemble of the classifiers using soft voting\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('lr', log_reg_best),\n",
        "    ('rf', rf_best)\n",
        "], voting='hard')\n",
        "# Train the ensemble on the training set\n",
        "ensemble.fit(X_train, y_train.values.ravel())"
      ],
      "metadata": {
        "id": "XEp43iU9ppKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "y_test_pred = ensemble.predict(X_test)\n",
        "# Calculate performance metrics\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
        "test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "# Print the performance metrics\n",
        "print(f\"Test accuracy: {test_accuracy}\")\n",
        "print(f\"Test precision: {test_precision}\")\n",
        "print(f\"Test recall: {test_recall}\")\n",
        "print(f\"Test f1: {test_f1}\")"
      ],
      "metadata": {
        "id": "loPvJjt6pHA3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}